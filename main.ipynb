{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d453e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Mon May 30 15:53:43 2022\n",
    "\n",
    "@author: a2326\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a081fb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_frame(width=None, height=None):\n",
    "    import matplotlib as mpl\n",
    "    mpl.rcParams['savefig.pad_inches'] = 0\n",
    "    figsize = None if width is None else (width, height)\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = plt.axes([0,0,1,1], frameon=False)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    plt.autoscale(tight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72df5613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import h5py\n",
    "import sklearn.decomposition\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "#from fcmeans import FCM\n",
    "#import skfuzzy as fuzz\n",
    "from sklearn.manifold import TSNE\n",
    "import collections\n",
    "from numpy.ma import masked_array\n",
    "from sklearn import mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c4778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(num):\n",
    "   res = \"\"\n",
    "   table = [\n",
    "      (1000, \"M\"),\n",
    "      (900, \"CM\"),\n",
    "      (500, \"D\"),\n",
    "      (400, \"CD\"),\n",
    "      (100, \"C\"),\n",
    "      (90, \"XC\"),\n",
    "      (50, \"L\"),\n",
    "      (40, \"XL\"),\n",
    "      (10, \"X\"),\n",
    "      (9, \"IX\"),\n",
    "      (5, \"V\"),\n",
    "      (4, \"IV\"),\n",
    "      (1, \"I\"),\n",
    "   ]\n",
    "   for cap, roman in table:\n",
    "      d, m = divmod(num, cap)\n",
    "      res += roman * d\n",
    "      num = m\n",
    "\n",
    "   return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ad2c97",
   "metadata": {},
   "source": [
    "#loading data##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de0b7b8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "##unfolding data##\n",
    "def unfold(data):\n",
    "    data_unfold=data.reshape(data.shape[0]*data.shape[1], data.shape[2])\n",
    "    return(data_unfold)\n",
    "##folding data##\n",
    "def fold(data):\n",
    "    data_fold=data.reshape(n,n, data_unfold.shape[1])\n",
    "    return(data_fold)\n",
    "##dark counts##shouldnt this happen after the alignment\n",
    "def df(data):\n",
    " darkcounts=np.zeros((data.shape[2]))\n",
    " for i in range(data.shape[2]):\n",
    "      darkcounts[i]=(data[0:20,0:20,i].mean()+data[0:20,n-20:n,i].mean()+data[n-20:n,0:20,i].mean()+data[n-20:n,n-20:n,i].mean())/4\n",
    "      data[:,:,i]=data[:,:,i]-darkcounts[i]\n",
    "      data[:,:,i]=np.where(data[:,:,i]<0, 0, data[:,:,i])\n",
    " return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c241b8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "##io normalization##\n",
    "def ionormalize(data,io):\n",
    "    data_norm=(data*np.mean(io))/io\n",
    "    return(data_norm)\n",
    "#normalization to bright field image\n",
    "def bfnormalize(data,norm):\n",
    "    data_bf=(data/norm[:,:,None])*np.mean(norm)\n",
    "    return(data_bf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8cf670",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform arrays#\n",
    "def transformd(data):\n",
    "    data=np.transpose(data,(1,0,2))\n",
    "    datanew=np.flipud(data)\n",
    "    return(datanew)\n",
    "def transform(data):\n",
    "    data=np.transpose(data)\n",
    "    datanew=np.flipud(data)\n",
    "    return(datanew)\n",
    "def transform1(data):\n",
    "    datanew=np.transpose(data,(1,0,2))\n",
    "    return(datanew)\n",
    "def transform2(data):\n",
    "    datanew=np.flipud(data)\n",
    "    return(datanew)\n",
    "def transform4(data):\n",
    "    datanew=np.transpose(np.flipud(data),(1,2,0))\n",
    "    return(datanew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c4addb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uni(records_array):\n",
    "    '''\n",
    "    get all indices of each unique value in array\n",
    "    '''\n",
    "    idx_sort = np.argsort(records_array)\n",
    "    sorted_records_array = records_array[idx_sort]\n",
    "    vals, idx_start, count = np.unique(sorted_records_array, return_counts=True, return_index=True)\n",
    "    res = np.split(idx_sort, idx_start[1:])\n",
    "    return dict(zip(vals, res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ae4f44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c27f4a7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def fov(data, resx, resy, vmin, vmax):\n",
    " font={'fontname':'Century Gothic'}\n",
    " plt.imshow(data, cmap='gray', extent=[0,data.shape[1]*resx,0,data.shape[0]*resy], vmin=vmin, vmax=vmax)\n",
    " plt.minorticks_on()\n",
    " plt.xlabel('x (μm)',weight=\"bold\",**font)\n",
    " plt.ylabel('y (μm)',weight=\"bold\",**font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653ae7e2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def scale(data, resx, resy,cmap):\n",
    " font={'fontname':'Century Gothic'}\n",
    " plt.imshow(data, cmap=cmap, extent=[0,data.shape[1]*resx,0,data.shape[0]*resy])\n",
    " #plt.xticks([])\n",
    " #plt.yticks([])\n",
    " plt.minorticks_on()\n",
    " plt.xlabel('x (μm)',weight=\"bold\",**font)\n",
    " plt.ylabel('y (μm)',weight=\"bold\",**font)\n",
    " plt.colorbar(format='%.1f',fraction=0.046, pad=0.04)#.set_label('Optical Density', rotation=90,weight=\"bold\", labelpad=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5506d137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noscale(data, resx, resy,cmap):\n",
    "  font={'fontname':'Century Gothic'}\n",
    "  plt.imshow(data, cmap=cmap, extent=[0,data.shape[1]*resx,0,data.shape[0]*resy])\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bb6917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignment(data):\n",
    " shift =np.zeros((2,data.shape[2]))\n",
    " error= np.zeros((data.shape[2]))\n",
    " diffphase=np.zeros((data.shape[2]))\n",
    " im_Cm=np.zeros((data.shape[0], data.shape[1], data.shape[2]))\n",
    "\n",
    " # align images with subpixel precision\n",
    " for i in range(data.shape[2]):\n",
    "     shift[:,i], error[i], diffphase[i] = phase_cross_correlation(data[:,:,0], data[:,:,i], upsample_factor=20)#20 is 0.05 pixels\n",
    "     im_Cm[:,:,i] = np.real( np.fft.ifftn(fourier_shift(np.fft.fftn(data[:,:,i]), shift[:,i])) )  \n",
    "     #print(f'Detected pixel offset (y, x): {shift[:,i]}')\n",
    "\n",
    " data_aligned=np.zeros((data.shape[0]-round(max(shift[0,:]))+round(min(shift[0,:])), data.shape[1]-round(max(shift[1,:]))+round(min(shift[1,:])), data.shape[2]))\n",
    " for i in range(data.shape[2]):\n",
    "    # data_aligned[:,:,i]=im_Cm[round(max(shift[0,:])):round(min(shift[0,:])), round(max(shift[1,:])):+round(min(shift[1,:])),i]\n",
    "    if  round(min(shift[0,:]))!=0 and round(min(shift[1,:]))!=0:\n",
    "        #print('case 1')\n",
    "        data_aligned[:,:,i]=im_Cm[round(max(shift[0,:])):round(min(shift[0,:])), round(max(shift[1,:])):round(min(shift[1,:])),i]\n",
    "    elif  round(min(shift[0,:]))==0 and round(min(shift[1,:]))!=0:\n",
    "        #print('case 2')\n",
    "        data_aligned[:,:,i]=im_Cm[round(max(shift[0,:])):, round(max(shift[1,:])):round(min(shift[1,:])),i]\n",
    "    elif  round(min(shift[0,:]))!=0 and round(min(shift[1,:]))==0:\n",
    "        #print('case 3')\n",
    "        data_aligned[:,:,i]=im_Cm[round(max(shift[0,:])):round(min(shift[0,:])), round(max(shift[1,:])):,i]\n",
    "    else:\n",
    "        #print('case 4')\n",
    "        data_aligned[:,:,i]=im_Cm[round(max(shift[0,:])):, round(max(shift[1,:])):,i]\n",
    " return shift, data_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a89a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heteroalignment(data,shift):\n",
    " im_Cm=np.zeros((data.shape[0], data.shape[1], data.shape[2]))\n",
    "\n",
    " for i in range(data.shape[2]):\n",
    "     im_Cm[:,:,i] = np.real( np.fft.ifftn(fourier_shift(np.fft.fftn(data[:,:,i]), shift[:,i])) )  \n",
    "     #print(f'Detected pixel offset (y, x): {shift[:,i]}')\n",
    "\n",
    " data_aligned=np.zeros((data.shape[0]-round(max(shift[0,:]))+round(min(shift[0,:])), data.shape[1]-round(max(shift[1,:]))+round(min(shift[1,:])), data.shape[2]))\n",
    " for i in range(data.shape[2]):\n",
    "    # data_aligned[:,:,i]=im_Cm[round(max(shift[0,:])):round(min(shift[0,:])), round(max(shift[1,:])):+round(min(shift[1,:])),i]\n",
    "    if  round(min(shift[0,:]))!=0 and round(min(shift[1,:]))!=0:\n",
    "        #print('case 1')\n",
    "        data_aligned[:,:,i]=im_Cm[round(max(shift[0,:])):round(min(shift[0,:])), round(max(shift[1,:])):round(min(shift[1,:])),i]\n",
    "    elif  round(min(shift[0,:]))==0 and round(min(shift[1,:]))!=0:\n",
    "        #print('case 2')\n",
    "        data_aligned[:,:,i]=im_Cm[round(max(shift[0,:])):, round(max(shift[1,:])):round(min(shift[1,:])),i]\n",
    "    elif  round(min(shift[0,:]))!=0 and round(min(shift[1,:]))==0:\n",
    "        #print('case 3')\n",
    "        data_aligned[:,:,i]=im_Cm[round(max(shift[0,:])):round(min(shift[0,:])), round(max(shift[1,:])):,i]\n",
    "    else:\n",
    "        #print('case 4')\n",
    "        data_aligned[:,:,i]=im_Cm[round(max(shift[0,:])):, round(max(shift[1,:])):,i]\n",
    " return data_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502f27e7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def norm(data, thresh):\n",
    "    data_unfold=unfold(data)\n",
    "    mask=np.zeros((data.shape[0]*data.shape[1]))\n",
    "    mask2=np.zeros((data.shape[0]*data.shape[1]))\n",
    "    data_mask=np.zeros((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "    data_mask2=np.zeros((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "    for j in range(data.shape[2]):\n",
    "     for i in range(data_unfold.shape[0]):\n",
    "        if np.mean(data_unfold[i,:])>thresh:\n",
    "            \n",
    "            mask[i]=1\n",
    "            mask2[i]=0\n",
    "            data_mask[i,j]=data_unfold[i,j]\n",
    "            data_mask2[i,j]=0\n",
    "        else:###pixels that dont belong to the background have 0 flux\n",
    "            mask[i]=0\n",
    "            mask2[i]=1\n",
    "            data_mask[i,j]=0\n",
    "            data_mask2[i,j]=data_unfold[i,j]\n",
    "    io_av=np.zeros((data.shape[2]))\n",
    "    for j in range(data.shape[2]):\n",
    "     io_av[j]=sum(data_mask[:,j])/sum(mask)\n",
    "    data_whitened=np.zeros((data.shape[0], data.shape[1], data.shape[2]))\n",
    "    data_whitened_alt=np.zeros((data.shape[0], data.shape[1], data.shape[2]))\n",
    "    for k in range(data.shape[2]):\n",
    "        data_whitened[:,:,k]=-np.log(abs(data[:,:,k]/io_av[k]))\n",
    "        #data_whitened_alt[:,:,k]=data[:,:,k]/io_av[k]\n",
    "        #print(k, data[:,:,k], io_av[k])\n",
    "        #data_whitened[:,:,k]=data[:,:,k]/(io_av[k]/sum(mask))\n",
    "    data_whitened_unfold=unfold(data_whitened)\n",
    "    return data_whitened, data_whitened_unfold,mask,io_av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80c0ce7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def normtey(data, thresh1, thresh2):\n",
    "    data_unfold=unfold(data)\n",
    "    mask=np.zeros((data.shape[0]*data.shape[1]))\n",
    "    mask2=np.zeros((data.shape[0]*data.shape[1]))\n",
    "    data_mask=np.zeros((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "    data_mask2=np.zeros((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "    for j in range(data.shape[2]):\n",
    "     for i in range(data_unfold.shape[0]):\n",
    "        if (np.mean(data_unfold[i,:])>thresh1) and (np.mean(data_unfold[i,:])<thresh2):\n",
    "            \n",
    "            mask[i]=1\n",
    "            mask2[i]=0\n",
    "            data_mask[i,j]=data_unfold[i,j]\n",
    "            data_mask2[i,j]=0\n",
    "        else:###pixels that dont belong to the background have 0 flux\n",
    "            mask[i]=0\n",
    "            mask2[i]=1\n",
    "            data_mask[i,j]=0\n",
    "            data_mask2[i,j]=data_unfold[i,j]\n",
    "    io_av=np.zeros((data.shape[2]))\n",
    "    for j in range(data.shape[2]):\n",
    "     io_av[j]=sum(data_mask[:,j])\n",
    "    data_whitened=np.zeros((data.shape[0], data.shape[1], data.shape[2]))\n",
    "    for k in range(data.shape[2]):\n",
    "        data_whitened[:,:,k]=data[:,:,k]/abs((io_av[k]/sum(mask)))\n",
    "        #data_whitened[:,:,k]=data[:,:,k]/(io_av[k]/sum(mask))\n",
    "    data_whitened_unfold=unfold(data_whitened)\n",
    "    return data_whitened, data_whitened_unfold,mask,io_av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc8eb84",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def gif(data, folder):\n",
    " import imageio\n",
    " imageio.mimsave(folder+'image.gif', np.transpose(data,(2,0,1)), fps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5ea584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pngtogif(energy, intensity, folder, labelx, labely):\n",
    " for i in range (int(intensity.shape[0])):\n",
    "        fig=plt.plot(energy[:i+1], intensity[:i+1])\n",
    "        plt.xlim([min(energy), max(energy)])\n",
    "        plt.minorticks_on()\n",
    "        plt.xlabel(labelx)\n",
    "        plt.ylabel(labely)\n",
    "       # plt.ylim([min(intensity)-0.1, max(intensity)+0.1])\n",
    "        plt.savefig(folder+str(i)+'.png')\n",
    " import imageio\n",
    " from PIL import Image\n",
    " images = []\n",
    " for i in range(0, int(energy.shape[0]), 1):    \n",
    "  print(i)\n",
    "  images.append(imageio.imread(folder+str(i)+'.png')) \n",
    "  imageio.mimsave(folder+'spectrum.gif', images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5261ed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seqgif(data, energy, folder, n1, n2, resy, resx):\n",
    " for i in range (n1, n2, 1):\n",
    "        fig=plt.imshow(data[:,:,i], cmap='gray', extent=[0,data.shape[1]*resy,0,data.shape[0]*resx])\n",
    "        plt.minorticks_on()\n",
    "        plt.title('E='+str(energy[i])+'eV')\n",
    "        plt.savefig(folder+str(i)+'.png')\n",
    "        plt.xlabel('y(um)')\n",
    "        plt.ylabel('x(um)')\n",
    " import imageio\n",
    " from PIL import Image\n",
    " images = []\n",
    " for i in range(n1, n2, 1):    \n",
    "  #print(i)\n",
    "  images.append(imageio.imread(folder+str(i)+'.png')) \n",
    "  imageio.mimsave(folder+'seq.gif', images, fps=1)\n",
    "\n",
    "\n",
    "def clusteringopt(data,nb_clusters, excl, energy,d,cl):\n",
    "    from sklearn.cluster import SpectralClustering\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import h5py\n",
    "    import sklearn.decomposition\n",
    "    from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "    #from fcmeans import FCM\n",
    "    #import skfuzzy as fuzz\n",
    "    from sklearn.manifold import TSNE\n",
    "    import collections\n",
    "    from numpy.ma import masked_array\n",
    "    from sklearn import mixture\n",
    "    data_unfold=data.reshape(data.shape[0]*data.shape[1],data.shape[2])\n",
    "    energy=energy\n",
    "    def uni(records_array):\n",
    "        '''\n",
    "        get all indices of each unique value in array\n",
    "        '''\n",
    "        idx_sort = np.argsort(records_array)\n",
    "        sorted_records_array = records_array[idx_sort]\n",
    "        vals, idx_start, count = np.unique(sorted_records_array, return_counts=True, return_index=True)\n",
    "        res = np.split(idx_sort, idx_start[1:])\n",
    "        return dict(zip(vals, res))\n",
    "\n",
    "    \n",
    "    nb_principle_comp = 6\n",
    "    exclude_first_component = excl\n",
    "    dim_reduction_method = d #'ICA'|'PCA'|'KernelPCA'|'NMF'|'TSNE'\n",
    "    cluster_method = cl #'kmeans'|'aggl_cl'|'DBSCAN'|'fcm'|'SpectralClustering'|'gmm'\n",
    "    skip_dim_reduction = False\n",
    "    normalize = False\n",
    "    #############################Dimensionality reduction############################################################################\n",
    "\n",
    "    if normalize:\n",
    "     # every value gets divided by the mean of its spectrum\n",
    "      data_unfold = (data_unfold/data_unfold.mean(0)[None,:])\n",
    "\n",
    "        ### Dimension Reduction ###\n",
    "    if not skip_dim_reduction:\n",
    "\n",
    "            if dim_reduction_method == 'PCA':\n",
    "                PCA=sklearn.decomposition.PCA\n",
    "                pca=PCA(n_components=0.999) # specify amount of variance by float value, chooses nb of components accordingly \n",
    "                H=pca.fit_transform(data_unfold)\n",
    "                H_refold=H.reshape(data.shape[0],data.shape[1],-1)\n",
    "                W=pca.components_.T\n",
    "\n",
    "            elif dim_reduction_method == 'ICA':\n",
    "                ICA=sklearn.decomposition.FastICA\n",
    "                ica=ICA(n_components=nb_principle_comp)\n",
    "                H=ica.fit_transform(data_unfold)\n",
    "                H_refold=H.reshape(data.shape[0],data.shape[1],-1)\n",
    "                W=ica.components_.T\n",
    "\n",
    "            elif dim_reduction_method == 'KernelPCA':\n",
    "                # https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html\n",
    "                PCA=sklearn.decomposition.KernelPCA\n",
    "                #pca=PCA(n_components=nb_principle_comp, kernel='rbf')\n",
    "                pca=PCA(n_components=nb_principle_comp, kernel='cosine', fit_inverse_transform = True)\n",
    "                #H=pca.fit_transform(od)\n",
    "                H=pca.fit_transform(data_unfold)\n",
    "                pca.H_transformed_fit_\n",
    "                H_refold=H.reshape(data.shape[0],data.shape[1],-1)\n",
    "                W=pca.components_.T\n",
    "            elif dim_reduction_method == 'NMF':\n",
    "                NMF=sklearn.decomposition.NMF\n",
    "                nmf=NMF(n_components=nb_principle_comp, max_iter=4000,  tol=1e-7)\n",
    "                #tol=1e-4 default,init == ‘nndsvdar’ or ‘random’ or None\n",
    "                data_unfold += abs(data_unfold.min()) # add minimum to every value to avoid negative values\n",
    "                H=nmf.fit_transform(data_unfold)\n",
    "                H_refold=H.reshape(data.shape[0],data.shape[1],-1)\n",
    "                W=nmf.components_.T\n",
    "            elif dim_reduction_method == 'TSNE':\n",
    "               #TSNE=sklearn.manifold.TSNE\n",
    "               #methods[\"t-SNE\"] = manifold.TSNE(n_components=2, init=\"pca\", random_state=0)\n",
    "               #tsne= TSNE(n_components=2, learning_rate='auto', init='random')\n",
    "               #H=tsne.fit_transform(od)\n",
    "                H = TSNE(n_components=nb_principle_comp , init='random', random_state=0).fit_transform(data_unfold)\n",
    "                H_refold=H.reshape(data.shape[0],data.shape[1],-1)\n",
    "               \n",
    "\n",
    "    ###########################Clustering###########################################\n",
    "    if cluster_method=='kmeans':\n",
    "            model = KMeans(n_clusters=nb_clusters, max_iter=5000,tol=0.0000001)\n",
    "    elif cluster_method=='aggl_cl':\n",
    "            model = KMeans(n_clusters=nb_clusters)\n",
    "            model = AgglomerativeClustering(n_clusters=nb_clusters, affinity='cosine', linkage='average')\n",
    "    elif cluster_method=='DBSCAN':\n",
    "            model = DBSCAN()\n",
    "    #elif cluster_method=='fcm':\n",
    "            #model = FCM(n_clusters=nb_clusters) # we use two cluster as an example\n",
    "    elif cluster_method=='gmm':\n",
    "            model = mixture.GaussianMixture(n_components=nb_clusters, covariance_type='full', random_state=42)\n",
    "    elif cluster_method=='SpectralClustering':\n",
    "         model=SpectralClustering(n_clusters=nb_clusters, eigen_solver=\"arpack\", affinity=\"nearest_neighbors\")\n",
    "\n",
    "    if not skip_dim_reduction:\n",
    "            clusters = model.fit_predict(H[:,1:nb_principle_comp+1]) if exclude_first_component else model.fit_predict(H[:,:nb_principle_comp])\n",
    "            clusters_img = clusters.reshape(data.shape[0],data.shape[1]) \n",
    "\n",
    "    else:\n",
    "            clusters = model.fit_predict(data_unfold)\n",
    "            clusters_img = clusters.reshape(data.shape[0],data.shape[1])\n",
    "    #######kmeans seems pretty robust with noise and so I will not reduce the dimensionality of my data with PCA############\n",
    "    ############################indexing#############################\n",
    "#plt.figure(1)  \n",
    "#plt.imshow(data.mean(2), cmap='gray')\n",
    "#print('Original Image (mean)')\n",
    "#plt.show()\n",
    "    cluster_per_pixel = clusters_img\n",
    "    cluster_idx = uni(cluster_per_pixel.reshape(-1))\n",
    "\n",
    "        \n",
    "    cluster_plot = plt.imshow(cluster_per_pixel, cmap='jet')\n",
    "    #plt.imshow(cluster_per_pixel, 'jet')\n",
    "    scale(cluster_per_pixel,resox,resoy, 'jet')\n",
    "    print('Clustered Image')\n",
    "    used_colors = cluster_plot.cmap(cluster_plot.norm(np.unique(cluster_per_pixel)))\n",
    "    #plt.show()\n",
    "    plt.figure(2) \n",
    "    for i in cluster_idx:\n",
    "\n",
    "        idx = cluster_idx[i]\n",
    "        #print('Overall Mean of cluster {}:'.format(i), od_fold.mean(2).reshape(-1)[idx].mean())\n",
    "        #plt.plot(energy,od_fold.reshape(-1,data.shape[2])[idx].mean(0), c=used_colors[i], label='Cluster'+str(i))\n",
    "        plt.plot(energy,data.reshape(-1,data.shape[2])[idx].mean(0)/max(data.reshape(-1,data.shape[2])[idx].mean(0)), c=used_colors[i], label='Cluster'+str(i))#/max(data.reshape(-1,data.shape[2])[idx].mean(0)), c=used_colors[i], label='Cluster'+str(i))\n",
    "        #divide by /od_fold.reshape(-1,data.shape[2])[idx].mean(0)[62] to normalize\n",
    "        plt.xlabel('Incident Photon Energy (eV)')\n",
    "        #plt.ylabel('OD per pixel/ ODmax')\n",
    "        plt.ylabel('Normalized to the maximum value')\n",
    "        #plt.ylabel('Avergage Trans signal')\n",
    "        plt.minorticks_on()   \n",
    "       # plt.title('Normalized at last peak')\n",
    "    print('Mean Spectrum of each cluster')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "        \n",
    "    return  cluster_per_pixel,cluster_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db731b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusteringbio(data,nb_clusters, excl, energy,d,cl):\n",
    "    from sklearn.cluster import SpectralClustering\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import h5py\n",
    "    import sklearn.decomposition\n",
    "    from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "    #from fcmeans import FCM\n",
    "    #import skfuzzy as fuzz\n",
    "    from sklearn.manifold import TSNE\n",
    "    import collections\n",
    "    from numpy.ma import masked_array\n",
    "    from sklearn import mixture\n",
    "    data_unfold=data.reshape(data.shape[0]*data.shape[1],data.shape[2])\n",
    "    energy=energy\n",
    "    def uni(records_array):\n",
    "        '''\n",
    "        get all indices of each unique value in array\n",
    "        '''\n",
    "        idx_sort = np.argsort(records_array)\n",
    "        sorted_records_array = records_array[idx_sort]\n",
    "        vals, idx_start, count = np.unique(sorted_records_array, return_counts=True, return_index=True)\n",
    "        res = np.split(idx_sort, idx_start[1:])\n",
    "        return dict(zip(vals, res))\n",
    "\n",
    "    \n",
    "    nb_principle_comp = 6\n",
    "    exclude_first_component = excl\n",
    "    dim_reduction_method = d #'ICA'|'PCA'|'KernelPCA'|'NMF'|'TSNE'\n",
    "    cluster_method = cl #'kmeans'|'aggl_cl'|'DBSCAN'|'fcm'|'SpectralClustering'|'gmm'\n",
    "    skip_dim_reduction = False\n",
    "    normalize = False\n",
    "    #############################Dimensionality reduction############################################################################\n",
    "\n",
    "    if normalize:\n",
    "     # every value gets divided by the mean of its spectrum\n",
    "      data_unfold = (data_unfold/data_unfold.mean(0)[None,:])\n",
    "\n",
    "        ### Dimension Reduction ###\n",
    "    if not skip_dim_reduction:\n",
    "\n",
    "            if dim_reduction_method == 'PCA':\n",
    "                PCA=sklearn.decomposition.PCA\n",
    "                pca=PCA(n_components=0.999) # specify amount of variance by float value, chooses nb of components accordingly \n",
    "                H=pca.fit_transform(data_unfold)\n",
    "                H_refold=H.reshape(data.shape[0],data.shape[1],-1)\n",
    "                W=pca.components_.T\n",
    "\n",
    "            elif dim_reduction_method == 'ICA':\n",
    "                ICA=sklearn.decomposition.FastICA\n",
    "                ica=ICA(n_components=nb_principle_comp)\n",
    "                H=ica.fit_transform(data_unfold)\n",
    "                H_refold=H.reshape(data.shape[0],data.shape[1],-1)\n",
    "                W=ica.components_.T\n",
    "\n",
    "            elif dim_reduction_method == 'KernelPCA':\n",
    "                # https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html\n",
    "                PCA=sklearn.decomposition.KernelPCA\n",
    "                #pca=PCA(n_components=nb_principle_comp, kernel='rbf')\n",
    "                pca=PCA(n_components=nb_principle_comp, kernel='cosine', fit_inverse_transform = True)\n",
    "                #H=pca.fit_transform(od)\n",
    "                H=pca.fit_transform(data_unfold)\n",
    "                pca.H_transformed_fit_\n",
    "                H_refold=H.reshape(data.shape[0],data.shape[1],-1)\n",
    "                W=pca.components_.T\n",
    "            elif dim_reduction_method == 'NMF':\n",
    "                NMF=sklearn.decomposition.NMF\n",
    "                nmf=NMF(n_components=nb_principle_comp, max_iter=4000,  tol=1e-7)\n",
    "                #tol=1e-4 default,init == ‘nndsvdar’ or ‘random’ or None\n",
    "                data_unfold += abs(data_unfold.min()) # add minimum to every value to avoid negative values\n",
    "                H=nmf.fit_transform(data_unfold)\n",
    "                H_refold=H.reshape(data.shape[0],data.shape[1],-1)\n",
    "                W=nmf.components_.T\n",
    "            elif dim_reduction_method == 'TSNE':\n",
    "               #TSNE=sklearn.manifold.TSNE\n",
    "               #methods[\"t-SNE\"] = manifold.TSNE(n_components=2, init=\"pca\", random_state=0)\n",
    "               #tsne= TSNE(n_components=2, learning_rate='auto', init='random')\n",
    "               #H=tsne.fit_transform(od)\n",
    "                H = TSNE(n_components=nb_principle_comp , init='random', random_state=0).fit_transform(data_unfold)\n",
    "                H_refold=H.reshape(data.shape[0],data.shape[1],-1)\n",
    "               \n",
    "\n",
    "    ###########################Clustering###########################################\n",
    "    if cluster_method=='kmeans':\n",
    "            model = KMeans(n_clusters=nb_clusters, max_iter=5000,tol=0.0000001)\n",
    "    elif cluster_method=='aggl_cl':\n",
    "            model = KMeans(n_clusters=nb_clusters)\n",
    "            model = AgglomerativeClustering(n_clusters=nb_clusters, affinity='cosine', linkage='average')\n",
    "    elif cluster_method=='DBSCAN':\n",
    "            model = DBSCAN()\n",
    "    #elif cluster_method=='fcm':\n",
    "            #model = FCM(n_clusters=nb_clusters) # we use two cluster as an example\n",
    "    elif cluster_method=='gmm':\n",
    "            model = mixture.GaussianMixture(n_components=nb_clusters, covariance_type='full', random_state=42)\n",
    "    elif cluster_method=='SpectralClustering':\n",
    "         model=SpectralClustering(n_clusters=nb_clusters, eigen_solver=\"arpack\", affinity=\"nearest_neighbors\")\n",
    "\n",
    "    if not skip_dim_reduction:\n",
    "            clusters = model.fit_predict(H[:,1:nb_principle_comp+1]) if exclude_first_component else model.fit_predict(H[:,:nb_principle_comp])\n",
    "            clusters_img = clusters.reshape(data.shape[0],data.shape[1]) \n",
    "\n",
    "    else:\n",
    "            clusters = model.fit_predict(data_unfold)\n",
    "            clusters_img = clusters.reshape(data.shape[0],data.shape[1])\n",
    "    #######kmeans seems pretty robust with noise and so I will not reduce the dimensionality of my data with PCA############\n",
    "    ############################indexing#############################\n",
    "#plt.figure(1)  \n",
    "#plt.imshow(data.mean(2), cmap='gray')\n",
    "#print('Original Image (mean)')\n",
    "#plt.show()\n",
    "    cluster_per_pixel = clusters_img\n",
    "    cluster_idx = uni(cluster_per_pixel.reshape(-1))\n",
    "\n",
    "        \n",
    "    cluster_plot = plt.imshow(cluster_per_pixel, cmap='jet')\n",
    "    #plt.imshow(cluster_per_pixel, 'jet')\n",
    "    plt.imshow(cluster_per_pixel, 'jet')\n",
    "    print('Clustered Image')\n",
    "    used_colors = cluster_plot.cmap(cluster_plot.norm(np.unique(cluster_per_pixel)))\n",
    "    #plt.show()\n",
    "    plt.figure(2) \n",
    "    for i in cluster_idx:\n",
    "\n",
    "        idx = cluster_idx[i]\n",
    "        #print('Overall Mean of cluster {}:'.format(i), od_fold.mean(2).reshape(-1)[idx].mean())\n",
    "        #plt.plot(energy,od_fold.reshape(-1,data.shape[2])[idx].mean(0), c=used_colors[i], label='Cluster'+str(i))\n",
    "        plt.plot(energy,data.reshape(-1,data.shape[2])[idx].mean(0)/max(data.reshape(-1,data.shape[2])[idx].mean(0)), c=used_colors[i], label='Cluster'+str(i))#/max(data.reshape(-1,data.shape[2])[idx].mean(0)), c=used_colors[i], label='Cluster'+str(i))\n",
    "        #divide by /od_fold.reshape(-1,data.shape[2])[idx].mean(0)[62] to normalize\n",
    "        plt.xlabel('Incident Photon Energy (eV)')\n",
    "        #plt.ylabel('OD per pixel/ ODmax')\n",
    "        plt.ylabel('Normalized to the maximum value')\n",
    "        #plt.ylabel('Avergage Trans signal')\n",
    "        plt.minorticks_on()   \n",
    "       # plt.title('Normalized at last peak')\n",
    "    print('Mean Spectrum of each cluster')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "        \n",
    "    return  cluster_per_pixel,cluster_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35545995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(data,nb_clusters, excl, energy):\n",
    "    from sklearn.cluster import SpectralClustering\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import h5py\n",
    "    import sklearn.decomposition\n",
    "    from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "    #from fcmeans import FCM\n",
    "    #import skfuzzy as fuzz\n",
    "    from sklearn.manifold import TSNE\n",
    "    import collections\n",
    "    from numpy.ma import masked_array\n",
    "    from sklearn import mixture\n",
    "    data_unfold=data.reshape(data.shape[0]*data.shape[1],data.shape[2])\n",
    "    energy=energy\n",
    "    def uni(records_array):\n",
    "        '''\n",
    "        get all indices of each unique value in array\n",
    "        '''\n",
    "        idx_sort = np.argsort(records_array)\n",
    "        sorted_records_array = records_array[idx_sort]\n",
    "        vals, idx_start, count = np.unique(sorted_records_array, return_counts=True, return_index=True)\n",
    "        res = np.split(idx_sort, idx_start[1:])\n",
    "        return dict(zip(vals, res))\n",
    "\n",
    "    \n",
    "    nb_principle_comp = 6\n",
    "    exclude_first_component = excl\n",
    "    dim_reduction_method = 'PCA' #'ICA'|'PCA'|'KernelPCA'|'NMF'|'TSNE'\n",
    "    cluster_method = 'kmeans' #'kmeans'|'aggl_cl'|'DBSCAN'|'fcm'|'SpectralClustering'|'gmm'\n",
    "    skip_dim_reduction = False\n",
    "    normalize = False\n",
    "    #############################Dimensionality reduction############################################################################\n",
    "\n",
    "    if normalize:\n",
    "     # every value gets divided by the mean of its spectrum\n",
    "      data_unfold = (data_unfold/data_unfold.mean(0)[None,:])\n",
    "\n",
    "        ### Dimension Reduction ###\n",
    "    if not skip_dim_reduction:\n",
    "\n",
    "            if dim_reduction_method == 'PCA':\n",
    "                PCA=sklearn.decomposition.PCA\n",
    "                pca=PCA(n_components=0.999) # specify amount of variance by float value, chooses nb of components accordingly \n",
    "                H=pca.fit_transform(data_unfold)\n",
    "                H_refold=H.reshape(data.shape[0],data.shape[1],-1)\n",
    "                W=pca.components_.T\n",
    "\n",
    "            elif dim_reduction_method == 'ICA':\n",
    "                ICA=sklearn.decomposition.FastICA\n",
    "                ica=ICA(n_components=nb_principle_comp)\n",
    "                H=ica.fit_transform(data_unfold)\n",
    "                H_refold=H.reshape(data.shape[0],data.shape[1],-1)\n",
    "                W=ica.components_.T\n",
    "\n",
    "            elif dim_reduction_method == 'KernelPCA':\n",
    "                # https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html\n",
    "                PCA=sklearn.decomposition.KernelPCA\n",
    "                #pca=PCA(n_components=nb_principle_comp, kernel='rbf')\n",
    "                pca=PCA(n_components=nb_principle_comp, kernel='cosine', fit_inverse_transform = True)\n",
    "                #H=pca.fit_transform(od)\n",
    "                H=pca.fit_transform(data_unfold)\n",
    "                pca.H_transformed_fit_\n",
    "                H_refold=H.reshape(data.shape[0],data.shape[1],-1)\n",
    "                W=pca.components_.T\n",
    "            elif dim_reduction_method == 'NMF':\n",
    "                NMF=sklearn.decomposition.NMF\n",
    "                nmf=NMF(n_components=nb_principle_comp, max_iter=4000,  tol=1e-7)\n",
    "                #tol=1e-4 default,init == ‘nndsvdar’ or ‘random’ or None\n",
    "                data_unfold += abs(data_unfold.min()) # add minimum to every value to avoid negative values\n",
    "                H=nmf.fit_transform(data_unfold)\n",
    "                H_refold=H.reshape(data.shape[0],data.shape[1],-1)\n",
    "                W=nmf.components_.T\n",
    "            elif dim_reduction_method == 'TSNE':\n",
    "               #TSNE=sklearn.manifold.TSNE\n",
    "               #methods[\"t-SNE\"] = manifold.TSNE(n_components=2, init=\"pca\", random_state=0)\n",
    "               #tsne= TSNE(n_components=2, learning_rate='auto', init='random')\n",
    "               #H=tsne.fit_transform(od)\n",
    "               H = TSNE(n_components=nb_principle_comp , init='random', random_state=0).fit_transform(data_unfold)\n",
    "               H_refold=H.reshape(data.shape[0],data.shape[1],-1)\n",
    "               \n",
    "\n",
    "    ###########################Clustering###########################################\n",
    "    if cluster_method=='kmeans':\n",
    "            model = KMeans(n_clusters=nb_clusters, max_iter=5000,tol=0.0000001)\n",
    "    elif cluster_method=='aggl_cl':\n",
    "            model = KMeans(n_clusters=nb_clusters)\n",
    "            model = AgglomerativeClustering(n_clusters=nb_clusters, affinity='cosine', linkage='average')\n",
    "    elif cluster_method=='DBSCAN':\n",
    "            model = DBSCAN()\n",
    "    #elif cluster_method=='fcm':\n",
    "            #model = FCM(n_clusters=nb_clusters) # we use two cluster as an example\n",
    "    elif cluster_method=='gmm':\n",
    "            model = mixture.GaussianMixture(n_components=nb_clusters, covariance_type='full', random_state=42)\n",
    "    elif cluster_method=='SpectralClustering':\n",
    "         model=SpectralClustering(n_clusters=nb_clusters, eigen_solver=\"arpack\", affinity=\"nearest_neighbors\")\n",
    "\n",
    "    if not skip_dim_reduction:\n",
    "            clusters = model.fit_predict(H[:,1:nb_principle_comp+1]) if exclude_first_component else model.fit_predict(H[:,:nb_principle_comp])\n",
    "\n",
    "            clusters_img = clusters.reshape(data.shape[0],data.shape[1]) \n",
    "\n",
    "    else:\n",
    "            clusters = model.fit_predict(data_unfold)\n",
    "            clusters_img = clusters.reshape(data.shape[0],data.shape[1])\n",
    "    #######kmeans seems pretty robust with noise and so I will not reduce the dimensionality of my data with PCA############\n",
    "    ############################indexing#############################\n",
    "    cluster_per_pixel = clusters_img\n",
    "    cluster_idx = uni(cluster_per_pixel.reshape(-1))\n",
    "    #plt.figure(1)  \n",
    "    #plt.imshow(data.mean(2), cmap='gray')\n",
    "    #print('Original Image (mean)')\n",
    "    #plt.show()\n",
    "        \n",
    "    cluster_plot = plt.imshow(cluster_per_pixel, cmap='jet')\n",
    "    #plt.imshow(cluster_per_pixel, 'jet')\n",
    "    plt.imshow(cluster_per_pixel, 'jet')\n",
    "    print('Clustered Image')\n",
    "    used_colors = cluster_plot.cmap(cluster_plot.norm(np.unique(cluster_per_pixel)))\n",
    "    #plt.show()\n",
    "    plt.figure(2) \n",
    "    for i in cluster_idx:\n",
    "\n",
    "        idx = cluster_idx[i]\n",
    "        #print('Overall Mean of cluster {}:'.format(i), od_fold.mean(2).reshape(-1)[idx].mean())\n",
    "        #plt.plot(energy,od_fold.reshape(-1,data.shape[2])[idx].mean(0), c=used_colors[i], label='Cluster'+str(i))\n",
    "        plt.plot(energy,data.reshape(-1,data.shape[2])[idx].mean(0)/max(data.reshape(-1,data.shape[2])[idx].mean(0)), c=used_colors[i], label='Cluster'+str(i))#/max(data.reshape(-1,data.shape[2])[idx].mean(0)), c=used_colors[i], label='Cluster'+str(i))\n",
    "        #divide by /od_fold.reshape(-1,data.shape[2])[idx].mean(0)[62] to normalize\n",
    "        plt.xlabel('Incident Photon Energy (eV)')\n",
    "        #plt.ylabel('OD per pixel/ ODmax')\n",
    "        plt.ylabel('Normalized to the maximum value')\n",
    "        #plt.ylabel('Avergage Trans signal')\n",
    "        plt.minorticks_on()   \n",
    "       # plt.title('Normalized at last peak')\n",
    "    print('Mean Spectrum of each cluster')\n",
    "    plt.show()\n",
    "        \n",
    "    #return  cluster_per_pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef2ee8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def odclustering(f,nb_clusters, excl):\n",
    " data=f['exchange/data'][...]\n",
    " data=np.transpose(data,(1,0,2))\n",
    " data=np.flipud(data)\n",
    " energy=f['exchange/energy'][...]\n",
    " x,y=f['exchange/x'][...], f['exchange/y'][...]\n",
    " od=f['spectromicroscopy/optical_density'][...]\n",
    " od_fold=od.reshape(data.shape[0],data.shape[1],data.shape[2])\n",
    " od_fold=np.flipud(od_fold)\n",
    " plt.figure(0) \n",
    " plt.imshow(np.sum(od_fold,axis=2))\n",
    "###################################get all indices of each unique value in array#####################################################################################\n",
    " def uni(records_array):\n",
    "    '''\n",
    "    get all indices of each unique value in array\n",
    "    '''\n",
    "    idx_sort = np.argsort(records_array)\n",
    "    sorted_records_array = records_array[idx_sort]\n",
    "    vals, idx_start, count = np.unique(sorted_records_array, return_counts=True, return_index=True)\n",
    "    res = np.split(idx_sort, idx_start[1:])\n",
    "    return dict(zip(vals, res))\n",
    "\n",
    "\n",
    " nb_principle_comp = 3\n",
    " exclude_first_component = excl\n",
    " dim_reduction_method = 'PCA' #'ICA'|'PCA'|'KernelPCA'|'NMF'|'TSNE'\n",
    " cluster_method = 'kmeans' #'kmeans'|'aggl_cl'|'DBSCAN'|'fcm'|'SpectralClustering'|'gmm'\n",
    " skip_dim_reduction = False\n",
    " normalize = False\n",
    " plotpca=False\n",
    "#############################Dimensionality reduction############################################################################\n",
    "\n",
    " if normalize:\n",
    " # every value gets divided by the mean of its spectrum\n",
    "  od = (od/od.mean(0)[None,:])\n",
    "\n",
    "    ### Dimension Reduction ###\n",
    " if not skip_dim_reduction:\n",
    "\n",
    "        if dim_reduction_method == 'PCA':\n",
    "            PCA=sklearn.decomposition.PCA\n",
    "            pca=PCA(n_components=0.999) # specify amount of variance by float value, chooses nb of components accordingly \n",
    "            H=pca.fit_transform(od)\n",
    "            H_refold=H.reshape(data.shape[0],data.shape[1],-1)\n",
    "            W=pca.components_.T\n",
    "\n",
    "        elif dim_reduction_method == 'ICA':\n",
    "            ICA=sklearn.decomposition.FastICA\n",
    "            ica=ICA(n_components=nb_principle_comp)\n",
    "            H=ica.fit_transform(od)\n",
    "            H_refold=H.reshape(data.shape[0],data.shape[1],-1)\n",
    "            W=ica.components_.T\n",
    "\n",
    "        elif dim_reduction_method == 'KernelPCA':\n",
    "            # https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html\n",
    "            PCA=sklearn.decomposition.KernelPCA\n",
    "            #pca=PCA(n_components=nb_principle_comp, kernel='rbf')\n",
    "            pca=PCA(n_components=nb_principle_comp, kernel='cosine', fit_inverse_transform = True)\n",
    "            #H=pca.fit_transform(od)\n",
    "            H=pca.fit_transform(od)\n",
    "            pca.H_transformed_fit_\n",
    "            H_refold=H.reshape(data.shape[0],data.shape[1],-1)\n",
    "            W=pca.components_.T\n",
    "        elif dim_reduction_method == 'NMF':\n",
    "            NMF=sklearn.decomposition.NMF\n",
    "            nmf=NMF(n_components=nb_principle_comp, max_iter=4000,  tol=1e-7)\n",
    "            #tol=1e-4 default,init == ‘nndsvdar’ or ‘random’ or None\n",
    "            od += abs(od.min()) # add minimum to every value to avoid negative values\n",
    "            H=nmf.fit_transform(od)\n",
    "            H_refold=H.reshape(data.shape[0],data.shape[1],-1)\n",
    "            W=nmf.components_.T\n",
    "        elif dim_reduction_method == 'TSNE':\n",
    "           #TSNE=sklearn.manifold.TSNE\n",
    "           #methods[\"t-SNE\"] = manifold.TSNE(n_components=2, init=\"pca\", random_state=0)\n",
    "           #tsne= TSNE(n_components=2, learning_rate='auto', init='random')\n",
    "           #H=tsne.fit_transform(od)\n",
    "           H = TSNE(n_components=nb_principle_comp , init='random', random_state=0).fit_transform(od)\n",
    "           H_refold=H.reshape(data.shape[0],data.shape[1],-1)         \n",
    " ###########################Clustering###########################################\n",
    " if cluster_method=='kmeans':\n",
    "        model = KMeans(n_clusters=nb_clusters, max_iter=5000,tol=0.0000001)\n",
    " elif cluster_method=='aggl_cl':\n",
    "        model = KMeans(n_clusters=nb_clusters)\n",
    "        model = AgglomerativeClustering(n_clusters=nb_clusters, affinity='cosine', linkage='average')\n",
    " elif cluster_method=='DBSCAN':\n",
    "        model = DBSCAN()\n",
    " #elif cluster_method=='fcm':\n",
    "        #model = FCM(n_clusters=nb_clusters) # we use two cluster as an example\n",
    " elif cluster_method=='gmm':\n",
    "        model = mixture.GaussianMixture(n_components=nb_clusters, covariance_type='full', random_state=42)\n",
    " elif cluster_method=='SpectralClustering':\n",
    "     model=SpectralClustering(n_clusters=nb_clusters, eigen_solver=\"arpack\", affinity=\"nearest_neighbors\")\n",
    "\n",
    " if not skip_dim_reduction:\n",
    "        clusters = model.fit_predict(H[:,1:nb_principle_comp+1]) if exclude_first_component else model.fit_predict(H[:,:nb_principle_comp])\n",
    "        #clusters = model.cmeans(H[:,1:nb_principle_comp+1], c=nb_clusters, m=2 , error=1e-4, maxiter'=1000)\n",
    "       # clusters = model.cmeans_predict(H[:,1:nb_principle_comp+1], ncenters, 2, error=0.005, maxiter=1000, init=None)\n",
    "                                        #'cntr_trained', 'm', 'error', 'maxiter') \n",
    "        ##fuzzy##\n",
    "      #  q = model.fit(H[:,1:nb_principle_comp+1]) if exclude_first_component else model.fit(H[:,:nb_principle_comp])\n",
    "       # clusters = model.predict(H[:,1:nb_principle_comp+1]) if exclude_first_component else model.predict(H[:,:nb_principle_comp])\n",
    "        #clusters_img = clusters.reshape(data.shape[0],data.shape[1])\n",
    "        #centers = model.centers\n",
    "        ###########################\n",
    "        ##gmm##\n",
    "       # q = model.fit(H[:,1:nb_principle_comp+1]) if exclude_first_component else model.fit(H[:,:nb_principle_comp])\n",
    "       # clusters = model.predict(H[:,1:nb_principle_comp+1]) if exclude_first_component else model.predict(H[:,:nb_principle_comp])\n",
    "        clusters_img = clusters.reshape(data.shape[0],data.shape[1]) \n",
    "       # probs = model.predict_proba(H[:,1:nb_principle_comp+1]) if exclude_first_component else model.predict_proba(H[:,:nb_principle_comp])\n",
    "        #probs_fold=probs.reshape(data.shape[0],data.shape[1],nb_clusters)\n",
    "       # ###########################\n",
    " else:\n",
    "        ##fuzzy##\n",
    "       # clusters = model.fit(od)\n",
    "        #centers = model.centers\n",
    "        #labels = model.predict(od)\n",
    "        #clusters_img\n",
    "        ##gmm##\n",
    "       # clusters = model.fit(od)\n",
    "       # clusters_img = clusters.reshape(data.shape[0],data.shape[1])\n",
    "       # probs = model.predict_proba(od)\n",
    "       # probs_fold=probs.reshape(data.shape[0],data.shape[1],nb_clusters)\n",
    "       # print(probs[:5].round(3))\n",
    "        #########= labels.reshape(data.shape[0],data.shape[1])\n",
    "        #########\n",
    "        clusters = model.fit_predict(od)\n",
    "        clusters_img = clusters.reshape(data.shape[0],data.shape[1])\n",
    "#######kmeans seems pretty robust with noise and so I will not reduce the dimensionality of my data with PCA############\n",
    "############################indexing#############################\n",
    " cluster_per_pixel = np.flipud(clusters_img)\n",
    " cluster_idx = uni(cluster_per_pixel.reshape(-1))\n",
    "######################################################\n",
    "######################Clustered image################################################\n",
    " \n",
    "    \n",
    " cluster_plot = plt.imshow(cluster_per_pixel, cmap='jet')\n",
    " print('Clustered Image')\n",
    " used_colors = cluster_plot.cmap(cluster_plot.norm(np.unique(cluster_per_pixel)))\n",
    " plt.show()\n",
    "######################Clustered spectra################################################\n",
    " return od_fold.reshape(-1,data.shape[2])[cluster_idx[0]].mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b24e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fineplot(x,y): \n",
    " fig, ax2 = plt.subplots(1,1)\n",
    " plt.plot(x,y,color='gray',linewidth=3.0,label='Single-layered MXene')\n",
    " plt.rcParams['font.size'] = '18'\n",
    " plt.xlabel('Incident Photon Energy (eV)',weight=\"bold\", fontsize=19)\n",
    " matplotlib.rcParams.update({'font.size': 19})\n",
    " #plt.xlim([450,475])\n",
    " #plt.ylim([0.03,0.47])\n",
    " plt.ylabel('Intensity (arb.units)',weight=\"bold\", fontsize=19)\n",
    " plt.yticks([])\n",
    " plt.minorticks_on()   \n",
    " for axis in ['top','bottom','left','right']:\n",
    "    ax2.spines[axis].set_linewidth(2)\n",
    " ax2.tick_params(width=2)\n",
    " plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9548ae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusteringnew(data,nb_clusters, excl, energy,dim,cl):\n",
    "    from sklearn.cluster import SpectralClustering\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import h5py\n",
    "    import sklearn.decomposition\n",
    "    from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "    #from fcmeans import FCM\n",
    "    #import skfuzzy as fuzz\n",
    "    from sklearn.manifold import TSNE\n",
    "    import collections\n",
    "    from numpy.ma import masked_array\n",
    "    from sklearn import mixture\n",
    "    data_unfold=data.reshape(data.shape[0]*data.shape[1],data.shape[2])\n",
    "    energy=energy\n",
    "    def uni(records_array):\n",
    "        '''\n",
    "        get all indices of each unique value in array\n",
    "        '''\n",
    "        idx_sort = np.argsort(records_array)\n",
    "        sorted_records_array = records_array[idx_sort]\n",
    "        vals, idx_start, count = np.unique(sorted_records_array, return_counts=True, return_index=True)\n",
    "        res = np.split(idx_sort, idx_start[1:])\n",
    "        return dict(zip(vals, res))\n",
    "\n",
    "    \n",
    "    nb_principle_comp = 6\n",
    "    exclude_first_component = excl\n",
    "    dim_reduction_method = dim #'ICA'|'PCA'|'KernelPCA'|'NMF'|'TSNE'\n",
    "    cluster_method = cl #'kmeans'|'aggl_cl'|'DBSCAN'|'fcm'|'SpectralClustering'|'gmm'\n",
    "    skip_dim_reduction = False\n",
    "    normalize = False\n",
    "    #############################Dimensionality reduction############################################################################\n",
    "\n",
    "    if normalize:\n",
    "     # every value gets divided by the mean of its spectrum\n",
    "      data_unfold = (data_unfold/data_unfold.mean(0)[None,:])\n",
    "\n",
    "        ### Dimension Reduction ###\n",
    "    if not skip_dim_reduction:\n",
    "\n",
    "            if dim_reduction_method == 'PCA':\n",
    "                PCA=sklearn.decomposition.PCA\n",
    "                pca=PCA(n_components=0.999) # specify amount of variance by float value, chooses nb of components accordingly \n",
    "                H=pca.fit_transform(data_unfold)\n",
    "                H_refold=H.reshape(data.shape[0],data.shape[1],-1)\n",
    "                W=pca.components_.T\n",
    "\n",
    "            elif dim_reduction_method == 'ICA':\n",
    "                ICA=sklearn.decomposition.FastICA\n",
    "                ica=ICA(n_components=nb_principle_comp)\n",
    "                H=ica.fit_transform(data_unfold)\n",
    "                H_refold=H.reshape(data.shape[0],data.shape[1],-1)\n",
    "                W=ica.components_.T\n",
    "\n",
    "            elif dim_reduction_method == 'KernelPCA':\n",
    "                # https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html\n",
    "                PCA=sklearn.decomposition.KernelPCA\n",
    "                #pca=PCA(n_components=nb_principle_comp, kernel='rbf')\n",
    "                pca=PCA(n_components=nb_principle_comp, kernel='cosine', fit_inverse_transform = True)\n",
    "                #H=pca.fit_transform(od)\n",
    "                H=pca.fit_transform(data_unfold)\n",
    "                pca.H_transformed_fit_\n",
    "                H_refold=H.reshape(data.shape[0],data.shape[1],-1)\n",
    "                W=pca.components_.T\n",
    "            elif dim_reduction_method == 'NMF':\n",
    "                NMF=sklearn.decomposition.NMF\n",
    "                nmf=NMF(n_components=nb_principle_comp, max_iter=4000,  tol=1e-7)\n",
    "                #tol=1e-4 default,init == ‘nndsvdar’ or ‘random’ or None\n",
    "                data_unfold += abs(data_unfold.min()) # add minimum to every value to avoid negative values\n",
    "                H=nmf.fit_transform(data_unfold)\n",
    "                H_refold=H.reshape(data.shape[0],data.shape[1],-1)\n",
    "                W=nmf.components_.T\n",
    "            elif dim_reduction_method == 'TSNE':\n",
    "               #TSNE=sklearn.manifold.TSNE\n",
    "               #methods[\"t-SNE\"] = manifold.TSNE(n_components=2, init=\"pca\", random_state=0)\n",
    "               #tsne= TSNE(n_components=2, learning_rate='auto', init='random')\n",
    "               #H=tsne.fit_transform(od)\n",
    "               H = TSNE(n_components=nb_principle_comp , init='random', random_state=0).fit_transform(data_unfold)\n",
    "               H_refold=H.reshape(data.shape[0],data.shape[1],-1)\n",
    "               \n",
    "\n",
    "    ###########################Clustering###########################################\n",
    "    if cluster_method=='kmeans':\n",
    "            model = KMeans(n_clusters=nb_clusters, max_iter=5000,tol=0.0000001)\n",
    "    elif cluster_method=='aggl_cl':\n",
    "            model = KMeans(n_clusters=nb_clusters)\n",
    "            model = AgglomerativeClustering(n_clusters=nb_clusters, affinity='cosine', linkage='average')\n",
    "    elif cluster_method=='DBSCAN':\n",
    "            model = DBSCAN()\n",
    "    #elif cluster_method=='fcm':\n",
    "            #model = FCM(n_clusters=nb_clusters) # we use two cluster as an example\n",
    "    elif cluster_method=='gmm':\n",
    "            model = mixture.GaussianMixture(n_components=nb_clusters, covariance_type='full', random_state=42)\n",
    "    elif cluster_method=='SpectralClustering':\n",
    "         model=SpectralClustering(n_clusters=nb_clusters, eigen_solver=\"arpack\", affinity=\"nearest_neighbors\")\n",
    "\n",
    "    if not skip_dim_reduction:\n",
    "            clusters = model.fit_predict(H[:,1:nb_principle_comp+1]) if exclude_first_component else model.fit_predict(H[:,:nb_principle_comp])\n",
    "\n",
    "            clusters_img = clusters.reshape(data.shape[0],data.shape[1]) \n",
    "\n",
    "    else:\n",
    "            clusters = model.fit_predict(data_unfold)\n",
    "            clusters_img = clusters.reshape(data.shape[0],data.shape[1])\n",
    "    #######kmeans seems pretty robust with noise and so I will not reduce the dimensionality of my data with PCA############\n",
    "    ############################indexing#############################\n",
    "    cluster_per_pixel = clusters_img\n",
    "    cluster_idx = uni(cluster_per_pixel.reshape(-1))\n",
    "    #plt.figure(1)  \n",
    "    #plt.imshow(data.mean(2), cmap='gray')\n",
    "    #print('Original Image (mean)')\n",
    "    #plt.show()\n",
    "        \n",
    "    cluster_plot = plt.imshow(cluster_per_pixel, cmap='jet')\n",
    "    #plt.imshow(cluster_per_pixel, 'jet')\n",
    "    scale(cluster_per_pixel,resox,resoy, 'jet')\n",
    "    print('Clustered Image')\n",
    "    used_colors = cluster_plot.cmap(cluster_plot.norm(np.unique(cluster_per_pixel)))\n",
    "    #plt.show()\n",
    "    plt.figure(2) \n",
    "    for i in cluster_idx:\n",
    "\n",
    "        idx = cluster_idx[i]\n",
    "        #print('Overall Mean of cluster {}:'.format(i), od_fold.mean(2).reshape(-1)[idx].mean())\n",
    "        #plt.plot(energy,od_fold.reshape(-1,data.shape[2])[idx].mean(0), c=used_colors[i], label='Cluster'+str(i))\n",
    "        plt.plot(energy,data.reshape(-1,data.shape[2])[idx].mean(0)/max(data.reshape(-1,data.shape[2])[idx].mean(0)), c=used_colors[i], label='Cluster'+str(i))#/max(data.reshape(-1,data.shape[2])[idx].mean(0)), c=used_colors[i], label='Cluster'+str(i))\n",
    "        #divide by /od_fold.reshape(-1,data.shape[2])[idx].mean(0)[62] to normalize\n",
    "        plt.xlabel('Incident Photon Energy (eV)')\n",
    "        #plt.ylabel('OD per pixel/ ODmax')\n",
    "        plt.ylabel('Normalized to the maximum value')\n",
    "        #plt.ylabel('Avergage Trans signal')\n",
    "        plt.minorticks_on()  \n",
    "        plt.legend()\n",
    "       # plt.title('Normalized at last peak')\n",
    "    print('Mean Spectrum of each cluster')\n",
    "    plt.show()\n",
    "    \n",
    "        \n",
    "    return  cluster_per_pixel,cluster_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f09d26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
